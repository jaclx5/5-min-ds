---
title: Solving WORDLE with Information Theory and Regular Expressions
layout: post
author: jaclx5
---

_An automated solution to the WORDLE puzzle using an Information Theoretical approach and Rrgular expression filtering._

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

> Check also the [companion notebook](https://github.com/jaclx5/jaclx5.github.io/blob/master/notebooks/wordle/wordle.ipynb) and the [pywordlesover](https://github.com/jaclx5/pywordlesolver) package (more info at the [end of the post](#code)). 

[WORDLE](https://www.powerlanguage.co.uk/wordle/) is a simple, free, on-line, single-player _guess-the-word_ game [created by Josh Wardle](https://en.wikipedia.org/wiki/Wordle_(video_game)).

It is extremely simple: The player has 6 tries to guess an unknown 5-letter English word (the solution). After each try the game will show a colored square for each letter of the guess word:
- A <span style="color:white; background: #34b763;">GREEN</span> square if the letter is in the right position.
- A <span style="color:white; background: #caba56;">YELLOW</span> square if the solution contains the letter, but in a different position.
- A <span style="color:white; background: #777c7e;">GRAY</span> square if the solution does not contain the letter.

In the following example the player should guess the word "VIDEO" (which is hidden) and tries the word "OLDEN".

<div align="center">
	<img src="/images/wordle/wordle_fig_5.png" width="300"/>
</div>

Letters "D" and "E" appear in green as they are in the right place. Letter "O" exists in "VIDEO" but not in the first position. Finally letters "L" and "N" do not occur in "VIDEO".


# Solving the Game

In general, when facing any type of game, I usually take more fun thinking on how to program a solver, than playing the game myself. I try to compensate with computer power my extreme inability for all kinds of games. WORDLE is not an exception. In this post I'll describe my approach to solve it with code.

As in most of _guessing_ games, in particular guess the word games, __Information Theory__ tends to be a good approach to solve them.


# Information Theory Mini Primer

[Information Theory](https://en.wikipedia.org/wiki/Information_theory) is a whole mathematical field created by [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) in the 1940's. A brilliant mathematical idea, Information Theory was originally developed to study the best way to transmit information over a noisy channel, but quickly became an invaluable tool to solve all sorts of realated problems, in particular, a key tool in the development of modern computer science.

A primer on Information Theory is way out of the scope of this post. We will need, however, to grasp a few basic concepts before going into the game solution.

The basic idea of Information Theory (and amazing Shannon's intuition) is that we can quantify the _"Information"_ of any random event (e.g. a flip of a coin, the weather tomorrow, the menu choices on a restaurant, ...) with the following formula:

$$ I(X) = \sum_{i=1}^{n} - P(x_i) \times log_2(P(x_i)) $$

Here $$I(X)$$ stands for the quantity of information associated to the event $$X$$ (e.g. a throw of a dice); $$n$$ represents the number of possible outcomes of the event (e.g. 6); $$x_i$$ represents each individual outcome (e.g. dice's faces 1 through 6); and finally $$P(x_i)$$ is the probability of each individual outcome (e.g. in the case of the dice throw is $$1/6$$ for all outcomes).

Armed with this formula let's compute $$I(2SidedFairCoin)$$, i.e., the quantity of information associated with tossing a 2-sided fair coin:

<table class="data-table">
	<tr><th>$$i$$</th><th>$$x_i$$</th><th>$$P(x_i)$$</th><th>$$log_2(P(x_i))$$</th><th>$$- P(x_i) \times log_2(P(x_i))$$</th></tr>
	<tr><td>1</td>    <td>Heads</td>  <td>0.5</td>       <td>-1</td>               <td>0.5</td></tr>
	<tr><td>2</td>    <td>Tails</td>  <td>0.5</td>       <td>-1</td>               <td>0.5</td></tr>
</table>

Summing all entries of the right-most column we'll obtain:

$$ I(X) = (0.5 + 0.5) = 1_{bits}$$

As a lateral remark note that the quantity of information is usually expressed in $$bits$$. This establishes a fundamental connection with binary coding and the way digital systems encode data. But I digress here...

Another important observation is that **_if_** the coin was not fair, its quantity of information would have been less than 1. The following table shows the information of several unfair coins for different probabilities of outcomes:

<table class="data-table">
	<tr><th>$$P(Heads)$$</th><th>$$P(Tails)$$</th><th>$$I(Coin)_{bits}$$</th></tr>
	<tr><td>$$0$$</td>       <td>$$1$$</td>       <td>$$0$$</td></tr>
	<tr><td>$$0.1$$</td>     <td>$$0.9$$</td>     <td>$$0.469$$</td></tr>
	<tr><td>$$0.2$$</td>     <td>$$0.8$$</td>     <td>$$0.722$$</td></tr>
	<tr><td>$$0.3$$</td>     <td>$$0.7$$</td>     <td>$$0.881$$</td></tr>
	<tr><td>$$0.4$$</td>     <td>$$0.6$$</td>     <td>$$0.971$$</td></tr>
	<tr><td>$$0.5$$</td>     <td>$$0.5$$</td>     <td>$$1$$</td></tr>
</table>

Note that, for brevity, we only show the first half of the table. It's easy to see that the remaining rows would mirror the rows from above due to the fact that $$P(Heads) = 1 - P(Tails)$$.

Intuitively, it is kind of obvious that if we know for sure that a given coin always lands on its Tails (first row of the table) we will not obtain any extra information form observing that it indeed landed on its tails, thus, the quantity of information obtained from tossing such a coin is zero. In summary, every (almost) sure event (e.g. tomorrow's Sunrise) is non informative.

A corollary of this definition of information - essential for our strategy - is that __the maximum amount of information associated with an event occurs when all its possible outcomes have the same probability__.




# WORDLE as an Information Theory Game

When devising a strategy to play WORDLE we should have in mind three assumptions:

__A)__ All the words played in the game, both solutions and guesses __must__ belong to the English dictionary. Although obvious, this is a very important point as it limits the universe of words to consider: A solution is always a word and not some random arrangement of 5 letters. Off course it also limits the universe of guess words. Thus, we can devise a game strategy as: To pick, at any given stage of the game, the _best_ word from a relatively small list (around 5000 words is small for a computer).

__B)__ At the begining of the game the solution is drawn randomly and every word has the same probability to be drawn. In reality we cannot prove this, but it is a good assumption that simplifies our life.

__C)__ The players' only source of information are the colored clues provided after each try.

Armed with these three assumptions, our _information theoretical_ game strategy will be:

1. Choose, from the list of possible words, the most informative one.
2. After receiving the response clues clean up the list of possible words (i.e. remove all words incompatible with the all clues received so far).
3. Repeat 1 and 2 until we guess the solution.


## The Most Informative Letters

Before jumping straight to the most informative word problem we will make a brief detour to solve a simpler problem: the most informative letter.

As an example, imagine that we play the letter "A" on the first position of the guess word. What is the probability of obtaining each of the 3 possible clues GREEN, YELLOW and GRAY?

To answer this question we only need to count how many words:
- Start with the letter "A" (GREEN).
- Contain but do not start with the letter "A" (YELLOW).
- Do not contain the letter "A" in any position (GRAY).

Can you see assumptions A), B) and C) playing a role here?

Lets actually compute those numbers for letters "A" and "Z" in the first position (in parenthesis the probability of each clue):

<table class="data-table">
	<tr><th>Clue</th><th>A in 1st pos.</th><th>Z in 1st pos.</th></tr>
	<tr><td><span style="color:white; background: #34b763;">GREEN </span></td> <td>228 (0.050)</td> <td>13 (0.003)</td></tr>
	<tr><td><span style="color:white; background: #caba56;">YELLOW</span></td> <td>1508 (0.328)</td> <td>75 (0.016)</td></tr>
	<tr><td><span style="color:white; background: #777c7e;">GRAY  </span></td> <td>2858 (0.622)</td> <td>4506 (0.981)</td></tr>
	<tr><td>TOTAL</td> <td>4594 (1.000)</td> <td>4594 (1.000)</td></tr>
</table>

As we already know from the formula we can compute how much information we obtain if we play those letters:

$$I(A_1) = - (0.05 \times log_2(0.05) + 0.328 \times log_2(0.328) + 0.622 \times log_2(0.622)) = 1.169$$

$$I(Z_1) = - (0.003 \times log_2(0.003) + 0.016 \times log_2(0.016) + 0.981 \times log_2(0.981)) = 0.151$$

It's easy to see that playing the letter "A" in the 1st position we will obtain much more information than playing letter "Z" in the same position.

Repeating the same process we can search for the most informative (MI) letter for each of the 5 positions:

<table class="data-table">
	<tr><th>Position</th><th>MI Letter</th><th>$$I(x_i)$$</th></tr>
	<tr><td>1</td>       <td>S</td> <td>1.42</td></tr>
	<tr><td>2</td>       <td>E</td> <td>1.37</td></tr>
	<tr><td>3</td>       <td>A</td> <td>1.29</td></tr>
	<tr><td>4</td>       <td>E</td> <td>1.46</td></tr>
	<tr><td>5</td>       <td>S</td> <td>1.45</td></tr>
</table>

Thus, the most informative sequence of letters is:

S E A E S

Unfortunately, for two distinct reasons, we can't and shouldn't use this sequence to start a WORDLE game.

The first reason is obvious: We can't play "SEAES" because it is not a word.

However, even if "SEAES" was a word, we probably should not use it. The reason for that is more subtle: When computing the most informative letter for each position we are implicitly assuming that each position is independent, i.e., we are assuming that the occurrence of a given letter in one position doesn't affect the occurrence of letters in other positions. Which is not true! We can verify it with a simple observation: There are almost twice ($${2443 \over 1266} = 1.93$$) as many "E" than "T" in all words, however, after an "A" we find 12.6 ($${151 \over 12} = 12.58$$) times more "T" than "E"! The occurrence of "E" is thus strongly (negatively) correlated with "A".

Thus, if we would compute the word that maximizes the sum of information, _independently_ computed, for each position (which, by the way is the word "SAREE") we have no guarantee of obtaining the most informative word (and in fact "SAREE" is not).

Fortunately not everything is lost. If we extend this same approach we can easily find the most informative word.


## The Most Informative Word?

To exemplify how we can extend the concept discussed above to the search of the most informative word let's follow an example: When playing the word "DRINK" what are all the possible response clues one gets and how probable are they?

Remember that we don't care about the specific solution, we are assuming that all solutions are possible and equally probable.

We will get the clue <span style="color:white; background: #34b763;">D</span> <span style="color:white; background: #34b763;">R</span> <span style="color:white; background: #34b763;">I</span> <span style="color:white; background: #34b763;">N</span> <span style="color:white; background: #34b763;">K</span> if the solution is precisely the word "DRINK", and this will occur with a probability of $${1 \over 4954} = 0.0002$$.


On the opposite case we will get the clue <span style="color:white; background: #777c7e;">D</span> <span style="color:white; background: #777c7e;">R</span> <span style="color:white; background: #777c7e;">I</span> <span style="color:white; background: #777c7e;">N</span> <span style="color:white; background: #777c7e;">K</span> if none of the letters D, I, K, N or R exist in the solution. This clue will occur with a probability of $${1209 \over 4954} = 0.244$$.



If we enumerate all possible outcomes and the respective probabilities for the word DRINK, we will obtain a huge table like this:

<table class="data-table">
	<tr><th>Clue</th>  <th>Count</th><th>Prob</th></tr>
	<tr><td>
		<span style="color:white; background: #34b763;">D</span>&nbsp;
		<span style="color:white; background: #34b763;">R</span>&nbsp;
		<span style="color:white; background: #34b763;">I</span>&nbsp; 
		<span style="color:white; background: #34b763;">N</span>&nbsp;
		<span style="color:white; background: #34b763;">K</span></td>
		<td>1</td> <td>0.0002</td></tr>
	<tr><td>
		<span style="color:white; background: #34b763;">D</span>&nbsp;
		<span style="color:white; background: #34b763;">R</span>&nbsp;
		<span style="color:white; background: #34b763;">I</span>&nbsp; 
		<span style="color:white; background: #777c7e;">N</span>&nbsp;
		<span style="color:white; background: #777c7e;">K</span></td>
		<td>8</td>    <td>0.002</td></tr>
	<tr><td>...</td> <td>...</td>    <td>...</td></tr>
	<tr><td>
		<span style="color:white; background: #777c7e;">D</span>&nbsp;
		<span style="color:white; background: #34b763;">R</span>&nbsp;
		<span style="color:white; background: #777c7e;">I</span>&nbsp; 
		<span style="color:white; background: #caba56;">N</span>&nbsp;
		<span style="color:white; background: #777c7e;">K</span></td>
		<td>14</td>    <td>0.003</td></tr>
	<tr><td>
		<span style="color:white; background: #777c7e;">D</span>&nbsp;
		<span style="color:white; background: #777c7e;">R</span>&nbsp;
		<span style="color:white; background: #777c7e;">I</span>&nbsp; 
		<span style="color:white; background: #777c7e;">N</span>&nbsp;
		<span style="color:white; background: #777c7e;">K</span></td>
		<td>1209</td> <td>0.244</td></tr>
	<tr><td>TOTAL</td> <td>4594</td> <td>1.000</td></tr>
</table>

Each entry of this table is an outcome of our random event. From this it is straightforward to compute the quantity of information of the word "DRINK" using the information formula above:

$$I(DRINK) = 4.492$$

Now we only have to repeat the same process for all words in the dictionary to obtain the most informative word of all (and also the least informative by sake of curiosity). And the winner is "TARES".

$$I(TARES) = 6.22$$

$$I(FUZZY) = 2.18$$

The low quantity of information of the word "FUZZY" is not surprising given the two Z's and the Y which are particularly uninformative letters.

The word "TARES", however, was unexpected to me. I would expect a word ending in "ES", The "T" in the beginning, however, was a mild surprise.


# The Strategy in Practice

Now we can put in place our strategy to play WORDLE with success. To illustrate it let's try to guess solution word "VIDEO" which we will pretend that we don't know.

## 1st Try

Our first move will be to play the most informative word "TARES" to obtain:

<div align="center">
	<img src="/images/wordle/wordle_fig_1.png" width="300"/>
</div>

From this clue we can select all the words that match the following regular expression:

<pre>
<code class="python">
	r"[^ARST][^ARST][^ARST]E[^ARST]"
</code>
</pre>

## 2nd Try

The next best word, considering only the filtered list of words, is "OLDEN" with $$I(OLDEN) = 4.06$$:

<div align="center">
	<img src="/images/wordle/wordle_fig_2.png" width="300"/>
</div>

Again we narrow the remaining dictionary with the a _regex_:

<pre>
<code class="python">
	r"[^ALNORST][^ALNRST]DE[^ALNRST]"
</code>
</pre>

Note that we should include the "O" in the first position (we know that "O" does not appear there). In addition we can exclude all words that do not contain an "O" as we know it exists, we just don't know were.

## 3rd Try

The third word will be "CODED" with $$I(CODED) = 2.32$$:

<div align="center">
	<img src="/images/wordle/wordle_fig_3.png" width="300"/>
</div>

Re filter:

<pre>
<code class="python">
	r"[^ACDLNORST][^ACDLNORST]DE[^ACDLNRST]"
</code>
</pre>

## 4th Try

At this this stage the only remaining word in the list is "VIDEO", we go it in 4 tries:

<div align="center">
	<img src="/images/wordle/wordle_fig_4.png" width="300"/>
</div>


# Evaluate the Strategy

As a benchmark to evaluate how good this strategy is I decided to test it with the whole dictionary. The average number of tries was 4.001. The strategy failed to solve 168 words (3.7% of the total) inside the 6 tries limit, with 1 word requiring 11 tries to be solved ("YEARS" was the nasty one).

To compare with other strategies I tested a random one (i.e. each word is chose at random from the filtered list at each trial) and the sum of most informative letters as discussed above:

<table class="data-table">
	<tr><th>Strategy</th>                <th>Average</th><th>Failed</th>    <th>Max</th></tr>
	<tr><td>Random</td>                  <td>4.609</td>  <td>392 (8.5%)</td><td>12 ("LULLS")</td></tr>
	<tr><td>Most Informative Letters</td><td>4.469</td>  <td>349 (7.6%)</td><td>12 ("PULLS")</td></tr>
	<tr><td>Most Informative Word</td>   <td>4.001</td>  <td>168 (3.7%)</td><td>11 ("YEARS")</td></tr>
</table>

As expected the most informative word was the best strategy by far. Surprisingly the random strategy is very close to the sum of most informative letters (sometimes even better). This fact would deserve some consideration by itself, but such analysis is beyond the scope of this post.

Is this the best strategy? I honestly don't know. I believe some cleverer approach would be able improve on this.


In any case this seems to be a pretty reasonable approach, being successful for more then 96% of the words, and, most important, it's a neat demonstration of the power and elegance of Information Theory as a problem solving tool.

<a name="code"/>
# Links and Software

You can reproduce all the results from this post using the [companion notebook](https://github.com/jaclx5/jaclx5.github.io/blob/master/notebooks/wordle/wordle.ipynb).

I also made available the package [pywordlesover](https://github.com/jaclx5/pywordlesolver) which allows you to solve WORDLE puzzles, to play WORDLE on the command line and benchmark the different solution strategies discussed in this post. The code is free to download and play with the code.

If you have questions or ideas feel free to share them in the comments box. 